{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-04T02:31:16.051488Z",
     "iopub.status.busy": "2024-12-04T02:31:16.051103Z",
     "iopub.status.idle": "2024-12-04T02:32:53.961183Z",
     "shell.execute_reply": "2024-12-04T02:32:53.960189Z",
     "shell.execute_reply.started": "2024-12-04T02:31:16.051448Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface langchain_ollama langchain langchain_community\n",
    "!pip install openai\n",
    "!pip install pinecone\n",
    "!curl curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:33:34.145532Z",
     "iopub.status.busy": "2024-12-04T02:33:34.145185Z",
     "iopub.status.idle": "2024-12-04T02:33:37.153472Z",
     "shell.execute_reply": "2024-12-04T02:33:37.152535Z",
     "shell.execute_reply.started": "2024-12-04T02:33:34.145503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3) # Wait for a few seconds for Ollama to load!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-04T02:36:37.308225Z",
     "iopub.status.busy": "2024-12-04T02:36:37.307860Z",
     "iopub.status.idle": "2024-12-04T02:37:28.962283Z",
     "shell.execute_reply": "2024-12-04T02:37:28.961436Z",
     "shell.execute_reply.started": "2024-12-04T02:36:37.308193Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama pull hf.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF:Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:33:42.778853Z",
     "iopub.status.busy": "2024-12-04T02:33:42.778509Z",
     "iopub.status.idle": "2024-12-04T02:33:42.782853Z",
     "shell.execute_reply": "2024-12-04T02:33:42.781991Z",
     "shell.execute_reply.started": "2024-12-04T02:33:42.778822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/kaggle/input/rag-code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:33:56.930114Z",
     "iopub.status.busy": "2024-12-04T02:33:56.929751Z",
     "iopub.status.idle": "2024-12-04T02:35:01.562228Z",
     "shell.execute_reply": "2024-12-04T02:35:01.561326Z",
     "shell.execute_reply.started": "2024-12-04T02:33:56.930084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi_with_reranker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:35:38.607392Z",
     "iopub.status.busy": "2024-12-04T02:35:38.606564Z",
     "iopub.status.idle": "2024-12-04T02:35:39.752420Z",
     "shell.execute_reply": "2024-12-04T02:35:39.751485Z",
     "shell.execute_reply.started": "2024-12-04T02:35:38.607351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "import datasets\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 0: Set up Pinecone index and embedding model\n",
    "pinecone_api_key = 'PINCONE_API_KEY'\n",
    "hf_api_token = \"HUGGINGFACE_API_TOKEN\"\n",
    "pinecone_namespace = 'web'\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index('iskku-data3')\n",
    "\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=hf_api_token,\n",
    ")\n",
    "\n",
    "# Step 1.5: Define the reranker model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "\n",
    "# Step 2: Retriever function to extract top_k matches from Pinecone\n",
    "def retrieve(query, top_k=5):\n",
    "    embedded_query = hf_embeddings.embed_query(query)\n",
    "    results = index.query(vector=embedded_query, top_k=top_k,namespace = pinecone_namespace, include_metadata=True)\n",
    "    return results['matches']\n",
    "\n",
    "# Step 3: Reranker function to reorder retrieved contexts\n",
    "def rerank(query, matches):\n",
    "    reranked_matches = []\n",
    "    inputs = []\n",
    "    for match in matches:\n",
    "        context_text = match['metadata']['text']\n",
    "        inputs.append((query, context_text))\n",
    "\n",
    "    # Tokenize and create input tensors\n",
    "    tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**tokenized_inputs).logits.squeeze()\n",
    "\n",
    "    # Attach scores to matches and sort\n",
    "    for i, match in enumerate(matches):\n",
    "        match['score'] = scores[i].item()\n",
    "    reranked_matches = sorted(matches, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    return reranked_matches\n",
    "\n",
    "\n",
    "# Step 4: Define the generator component using ChatOllama\n",
    "def generate_response(query, context):\n",
    "    llm = ChatOllama(model='hf.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF:Q4_K_M', temperature=0.1, endpoint = 'http://localhost:11434')\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = ChatPromptTemplate.from_template('''You are an AI assistant specialized in providing accurate and detailed information. \n",
    "    Based on the given context about SungKyunKwan University (성균관대학교), answer the following question thoroughly and concisely. \n",
    "    Respond in the same language as the user.\n",
    "\n",
    "Context: {context}\n",
    "Question: {query}\n",
    "Answer:''')\n",
    "\n",
    "    chain = prompt | llm\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"query\": \"query\",\n",
    "            \"context\": \"context\",\n",
    "        }\n",
    "    )\n",
    "    print(answer.content)\n",
    "    return answer.content\n",
    "\n",
    "# Step 4: RAG pipeline combining retriever and generator\n",
    "def rag_pipeline(query):\n",
    "    # Retrieve relevant contexts from Pinecone\n",
    "    matches = retrieve(query)\n",
    "    retrieved_context = \"\\n\".join([match['metadata']['text'] for match in matches])\n",
    "\n",
    "    # Generate answer based on retrieved context\n",
    "    response = generate_response(query, retrieved_context)\n",
    "    return response, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:35:43.676074Z",
     "iopub.status.busy": "2024-12-04T02:35:43.675411Z",
     "iopub.status.idle": "2024-12-04T02:35:43.681121Z",
     "shell.execute_reply": "2024-12-04T02:35:43.679951Z",
     "shell.execute_reply.started": "2024-12-04T02:35:43.676023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rag_pipeline_reranker(query):\n",
    "    # Retrieve relevant contexts from Pinecone\n",
    "    matches = retrieve(query)\n",
    "    \n",
    "    # Rerank the retrieved contexts\n",
    "    reranked_matches = rerank(query, matches)\n",
    "    retrieved_context = \"\\n\".join([match['metadata']['text'] for match in reranked_matches[:3]])\n",
    "\n",
    "    # Generate the response iterator\n",
    "    response = generate_response(query, retrieved_context)\n",
    "\n",
    "    # Return the StreamingResponse directly\n",
    "    return response, reranked_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:35:47.625811Z",
     "iopub.status.busy": "2024-12-04T02:35:47.625097Z",
     "iopub.status.idle": "2024-12-04T02:35:47.632969Z",
     "shell.execute_reply": "2024-12-04T02:35:47.632013Z",
     "shell.execute_reply.started": "2024-12-04T02:35:47.625755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    # try:  # load previous generations if they exist\n",
    "    #     with open(output_file, \"r\") as f:\n",
    "    #         outputs = json.load(f)\n",
    "    # except:\n",
    "    outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = rag_pipeline(question)\n",
    "\n",
    "         # Convert `ScoredVector` objects to serializable dictionaries\n",
    "        retrieved_docs = [\n",
    "            {\n",
    "                \"id\": doc.id,  # Document ID\n",
    "                \"score\": doc.score,  # Similarity score\n",
    "                \"metadata\": doc.metadata,  # Metadata from Pinecone\n",
    "            }\n",
    "            for doc in relevant_docs\n",
    "        ]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"context\": example[\"content\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:35:52.245996Z",
     "iopub.status.busy": "2024-12-04T02:35:52.245407Z",
     "iopub.status.idle": "2024-12-04T02:35:52.255318Z",
     "shell.execute_reply": "2024-12-04T02:35:52.254660Z",
     "shell.execute_reply.started": "2024-12-04T02:35:52.245961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:35:58.164856Z",
     "iopub.status.busy": "2024-12-04T02:35:58.164456Z",
     "iopub.status.idle": "2024-12-04T02:35:59.101684Z",
     "shell.execute_reply": "2024-12-04T02:35:59.101030Z",
     "shell.execute_reply.started": "2024-12-04T02:35:58.164790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "OPENAI_API_KEY = user_secrets.get_secret(\"Open_AI\")\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"GPT4o-mini\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    # if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "    #     answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T02:37:58.538658Z",
     "iopub.status.busy": "2024-12-04T02:37:58.538270Z",
     "iopub.status.idle": "2024-12-04T02:38:12.329809Z",
     "shell.execute_reply": "2024-12-04T02:38:12.328633Z",
     "shell.execute_reply.started": "2024-12-04T02:37:58.538624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "GENERATOR_NAME = \"EEVE-Korean-Instruct-10.8B-v1.0-GGUF:Q4_K_M\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from a local JSON file\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/eval-data/eval_dataset_final.json\")\n",
    "\n",
    "# Access the Dataset object\n",
    "eval_dataset = eval_dataset[\"train\"]  # The loaded file is stored in the \"train\" split\n",
    "\n",
    "settings_name = f\"chunk_embeddings_rerank:False_reader-model:{GENERATOR_NAME}\"\n",
    "output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "print(\"Loading knowledge base embeddings...\")\n",
    "\n",
    "print(\"Running RAG...\")\n",
    "run_rag_tests(\n",
    "    eval_dataset=eval_dataset,\n",
    "    output_file=output_file_name,\n",
    "    verbose=False,\n",
    "    test_settings=settings_name,\n",
    ")\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "evaluate_answers(\n",
    "    output_file_name,\n",
    "    eval_chat_model,\n",
    "    evaluator_name,\n",
    "    evaluation_prompt_template,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6174934,
     "sourceId": 10026929,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6180686,
     "sourceId": 10034739,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 209729673,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
