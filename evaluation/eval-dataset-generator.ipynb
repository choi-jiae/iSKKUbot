{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9877988,"sourceType":"datasetVersion","datasetId":6064588},{"sourceId":9884534,"sourceType":"datasetVersion","datasetId":6069781}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Install packages and dependencies","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install openai\n!pip install tiktoken\n!pip install sentence-transformers\n!pip install -U langchain-community\n!pip install ragatouille\n!pip install tqdm openpyxl pandas\n!pip install ipywidgets\n!pip install datasets\n!pip install frontend\n!pip install huggingface_hub\n!pip install json\n!pip install langchain-openai","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2024-11-13T10:34:01.889574Z","iopub.execute_input":"2024-11-13T10:34:01.890687Z","iopub.status.idle":"2024-11-13T10:36:52.212458Z","shell.execute_reply.started":"2024-11-13T10:34:01.890629Z","shell.execute_reply":"2024-11-13T10:36:52.211223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nimport pandas as pd\n\nfrom typing import Optional, List, Tuple\n\nimport json\n\nimport datasets\n\nimport os\n\npd.set_option(\"display.max_colwidth\", None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:36:59.214957Z","iopub.execute_input":"2024-11-13T10:36:59.215407Z","iopub.status.idle":"2024-11-13T10:37:00.683693Z","shell.execute_reply.started":"2024-11-13T10:36:59.215356Z","shell.execute_reply":"2024-11-13T10:37:00.682577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub.hf_api import HfFolder\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHfFolder.save_token(user_secrets.get_secret(\"HF_TOKEN\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:37:02.955344Z","iopub.execute_input":"2024-11-13T10:37:02.955925Z","iopub.status.idle":"2024-11-13T10:37:03.278400Z","shell.execute_reply.started":"2024-11-13T10:37:02.955887Z","shell.execute_reply":"2024-11-13T10:37:03.277237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# # Path to your JSONL file\n# jsonl_path = '/kaggle/input/scrapped-dataset/results.jsonl'\n\n# # Load JSONL data\n# def load_jsonl_data(filepath):\n#     data = []\n#     with open(filepath, 'r') as f:\n#         for line in f:\n#             data.append(json.loads(line))\n#     return data\n\n# # Load the data\n# data = load_jsonl_data(jsonl_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:37:26.211463Z","iopub.execute_input":"2024-11-12T13:37:26.212332Z","iopub.status.idle":"2024-11-12T13:37:26.233475Z","shell.execute_reply.started":"2024-11-12T13:37:26.212289Z","shell.execute_reply":"2024-11-12T13:37:26.232776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\nfrom langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n\n\nimport os\n\nimport bs4\n\n\nfrom getpass import getpass\n\nfrom uuid import uuid4\n\nfrom langchain.docstore.document import Document as LangchainDocument\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\ntext_splitter = RecursiveCharacterTextSplitter(\n\n    chunk_size=900,\n\n    chunk_overlap=150,\n\n    add_start_index=True,\n\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:51:33.821061Z","iopub.execute_input":"2024-11-13T11:51:33.821509Z","iopub.status.idle":"2024-11-13T11:51:33.828879Z","shell.execute_reply.started":"2024-11-13T11:51:33.821469Z","shell.execute_reply":"2024-11-13T11:51:33.827719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to JSONL file\njsonl_path = '/kaggle/input/scrapped-dataset/results.jsonl'\n\n# Load JSONL data\ndef load_jsonl_data(filepath):\n    data = []\n    with open(filepath, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\n# Read JSONL data and create LangchainDocument objects for each entry\ndata = load_jsonl_data(jsonl_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:51:36.830062Z","iopub.execute_input":"2024-11-13T11:51:36.830861Z","iopub.status.idle":"2024-11-13T11:51:36.849561Z","shell.execute_reply.started":"2024-11-13T11:51:36.830819Z","shell.execute_reply":"2024-11-13T11:51:36.847987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store extracted documents\ndocs = []\nfor entry in data:\n    # Use the 'content' field from each JSON entry as document text\n    text = entry.get('content', '')\n    if text:  # Ensure there's text to process\n        docs.append(LangchainDocument(page_content=text))\n\n# Process documents with the text splitter\ndocs_processed = []\nfor doc in docs:\n    docs_processed += text_splitter.split_documents([doc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:51:38.706226Z","iopub.execute_input":"2024-11-13T11:51:38.706642Z","iopub.status.idle":"2024-11-13T11:51:38.726088Z","shell.execute_reply.started":"2024-11-13T11:51:38.706601Z","shell.execute_reply":"2024-11-13T11:51:38.724856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"docs_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T05:36:32.380103Z","iopub.execute_input":"2024-11-13T05:36:32.380732Z","iopub.status.idle":"2024-11-13T05:36:32.416718Z","shell.execute_reply.started":"2024-11-13T05:36:32.380689Z","shell.execute_reply":"2024-11-13T05:36:32.415768Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"Open_AI\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:37:21.560674Z","iopub.execute_input":"2024-11-13T10:37:21.561088Z","iopub.status.idle":"2024-11-13T10:37:22.005831Z","shell.execute_reply.started":"2024-11-13T10:37:21.561049Z","shell.execute_reply":"2024-11-13T10:37:22.004236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from openai import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\n\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n)\n\ndef call_llm(prompt: str):\n    response = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"assistant\",\n                \"content\": prompt,\n            }\n        ],\n        model=\"gpt-4o-mini\",\n    )\n    return response.choices[0].message.content\n\n# Example call\nprint(call_llm(\"This is a test context\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:37:29.568749Z","iopub.execute_input":"2024-11-13T10:37:29.569173Z","iopub.status.idle":"2024-11-13T10:37:30.931110Z","shell.execute_reply.started":"2024-11-13T10:37:29.569133Z","shell.execute_reply":"2024-11-13T10:37:30.930035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"QA_generation_prompt = \"\"\"\n\nYour task is to write a factoid question and an answer given a context.\n\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\n\nYour factoid question should be formulated in the same style as questions users could ask in a search engine.\n\nYour factoid question should be related to Sungkyunkwan University (성균관대학교).\n\nThis means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n\n\n\nProvide your answer as follows:\n\n\n\nOutput:::\n\nFactoid question: (your factoid question)\n\nAnswer: (your answer to the factoid question)\n\n\n\nNow here is the context.\n\n\n\nContext: {context}\\n\n\nOutput:::\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:37:34.866647Z","iopub.execute_input":"2024-11-13T10:37:34.868018Z","iopub.status.idle":"2024-11-13T10:37:34.874238Z","shell.execute_reply.started":"2024-11-13T10:37:34.867954Z","shell.execute_reply":"2024-11-13T10:37:34.873053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"docs_processed.__len__()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:53:04.384260Z","iopub.execute_input":"2024-11-13T11:53:04.384697Z","iopub.status.idle":"2024-11-13T11:53:04.391686Z","shell.execute_reply.started":"2024-11-13T11:53:04.384657Z","shell.execute_reply":"2024-11-13T11:53:04.390585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nfrom tqdm import tqdm\n\nN_GENERATIONS = 331 # We intentionally generate only 10 QA couples here for cost and time considerations\n\n\n\nprint(f\"Generating {N_GENERATIONS} QA couples...\")\n\n\n\noutputs = []\n\nfor sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n\n    # Generate QA couple\n\n    output_QA_couple = call_llm(prompt = QA_generation_prompt.format(context=sampled_context.page_content))\n\n    # print(\"Generated output:\", output_QA_couple)\n\n    try:\n\n        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n\n        answer = output_QA_couple.split(\"Answer: \")[-1]\n\n        assert len(answer) < 300, \"Answer is too long\"\n\n        outputs.append(\n\n            {\n\n                \"context\": sampled_context.page_content,\n\n                \"question\": question,\n\n                \"answer\": answer,\n\n                # \"source_doc\": sampled_context.metadata[\"source\"],\n\n            }\n\n        )\n\n    except:\n\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:53:15.686395Z","iopub.execute_input":"2024-11-13T11:53:15.687140Z","iopub.status.idle":"2024-11-13T11:59:50.260189Z","shell.execute_reply.started":"2024-11-13T11:53:15.687095Z","shell.execute_reply":"2024-11-13T11:59:50.258739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(pd.DataFrame(outputs).head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:41:38.144796Z","iopub.execute_input":"2024-11-13T10:41:38.145728Z","iopub.status.idle":"2024-11-13T10:41:38.178456Z","shell.execute_reply.started":"2024-11-13T10:41:38.145684Z","shell.execute_reply":"2024-11-13T10:41:38.177333Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question_groundedness_critique_prompt = \"\"\"\n\nYou will be given a context and a question related to Sungkyunkwan University (성균관대학교).\n\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n\n\n\nProvide your answer as follows:\n\n\n\nAnswer:::\n\nEvaluation: (your rationale for the rating, as a text)\n\nTotal rating: (your rating, as a number between 1 and 5)\n\n\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\n\n\nNow here are the question and context.\n\n\n\nQuestion: {question}\\n\n\nContext: {context}\\n\n\nAnswer::: \"\"\"\n\n\n\nquestion_relevance_critique_prompt = \"\"\"\n\nYou will be given a question related to Sungkyunkwan University (성균관대학교).\n\nYour task is to provide a 'total rating' representing how useful this question can be to students and more specifically exchange students of Sungkyunkwan University (성균관대학교) .\n\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n\n\n\nProvide your answer as follows:\n\n\n\nAnswer:::\n\nEvaluation: (your rationale for the rating, as a text)\n\nTotal rating: (your rating, as a number between 1 and 5)\n\n\n\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n\n\n\nNow here is the question.\n\n\n\nQuestion: {question}\\n\n\nAnswer::: \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:16:47.190230Z","iopub.execute_input":"2024-11-13T11:16:47.190676Z","iopub.status.idle":"2024-11-13T11:16:47.198255Z","shell.execute_reply.started":"2024-11-13T11:16:47.190635Z","shell.execute_reply":"2024-11-13T11:16:47.197074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Generating critique for each QA couple...\")\n\nfor output in tqdm(outputs):\n\n    evaluations = {\n\n        \"groundedness\": call_llm(\n\n            prompt = question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n\n        ),\n\n        \"relevance\": call_llm(\n\n            prompt = question_relevance_critique_prompt.format(question=output[\"question\"]),\n\n        ),\n\n    }\n\n    try:\n\n        for criterion, evaluation in evaluations.items():\n\n            score, eval = (\n\n                int(evaluation.split(\"Total rating: \")[-1].strip()),\n\n                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n\n            )\n\n            output.update(\n\n                {\n\n                    f\"{criterion}_score\": score,\n\n                    f\"{criterion}_eval\": eval,\n\n                }\n\n            )\n\n    except Exception as e:\n\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:00:07.496188Z","iopub.execute_input":"2024-11-13T12:00:07.496661Z","iopub.status.idle":"2024-11-13T12:30:58.344895Z","shell.execute_reply.started":"2024-11-13T12:00:07.496615Z","shell.execute_reply":"2024-11-13T12:30:58.343876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n\npd.set_option(\"display.max_colwidth\", None)\n\n\n\ngenerated_questions = pd.DataFrame.from_dict(outputs)\n\n\n\nprint(\"Evaluation dataset before filtering:\")\n\ndisplay(\n\n    generated_questions[\n\n        [\n\n            \"question\",\n\n            \"answer\",\n\n            \"groundedness_score\",\n\n            \"relevance_score\",\n\n        ]\n\n    ]\n\n)\n\ngenerated_questions = generated_questions.loc[\n\n    (generated_questions[\"groundedness_score\"] >= 3)\n\n    & (generated_questions[\"relevance_score\"] >= 3)\n\n]\n\nprint(\"============================================\")\n\nprint(\"Final evaluation dataset:\")\n\ndisplay(\n\n    generated_questions[\n\n        [\n\n            \"question\",\n\n            \"answer\",\n\n            \"groundedness_score\",\n\n            \"relevance_score\",\n\n        ]\n\n    ]\n\n)\n\n\n\neval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:33:02.460056Z","iopub.execute_input":"2024-11-13T12:33:02.460464Z","iopub.status.idle":"2024-11-13T12:33:02.511374Z","shell.execute_reply.started":"2024-11-13T12:33:02.460428Z","shell.execute_reply":"2024-11-13T12:33:02.510351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset.to_csv('/kaggle/working/eval_dataset_5.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:33:20.163534Z","iopub.execute_input":"2024-11-13T12:33:20.164570Z","iopub.status.idle":"2024-11-13T12:33:20.198152Z","shell.execute_reply.started":"2024-11-13T12:33:20.164505Z","shell.execute_reply":"2024-11-13T12:33:20.197081Z"}},"outputs":[],"execution_count":null}]}
